---
title: "Market Segmentation and Preference Analysis"
author: "<b> CSDA1050 Group2 - Fanny, Deenu, Dave and Kaustubh </b>"
date: "04/26/2020"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    fig_caption: yes  
    fig_height: 6
    fig.align: center
    fig_width: 12
    df_print: paged
    keep_md: yes
    number_sections: yes
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: inline
---

Before loading the data, we start by installing and evoking all the necessary libraries.

```{r message=FALSE, warning=FALSE, include=FALSE}
#install.packages("plotly")
#install.packages("tidyverse")
#install.packages("lubridate")
#install.packages("stringi")
library(ggplot2)
library(plotly)
library(dplyr)
library(lubridate)
library(tidytext)
library(tidyverse)
library(plotly)
library(knitr)
library(data.table)
library(kableExtra)
library(ggpubr)
library(Rtsne)
library(gower)
```


```{r include=FALSE}
#This is a custom function to improve asthetics of ggplots. This code will not appear in output html.
#Function to lable x axis text to angle 90, and turn off legend.
theme_function = function() {
  theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=16, hjust=0.5), 
          axis.text.x = element_text(angle=90)
          )
}

show_table = function(df, x, isknit)
  if (isknit == TRUE) {
    knitr::kable(head(df, x))
  } else head(df, x)
```

# Introduction
Our primary dataset consists of 4 years of US Sales from the retail store `Superstore`. The data set was obtained from Kaggle at the following URL `https://www.kaggle.com/aksha17/superstore-sales`. This dataset is made of 21 variables over 9,994 rows, has 4 continuous variables (Sales, Quantity, Discount, and Profit), and the rest of the variables are categorical, and mainly consist of demographic and product information.

Let us begin with reading the data file downloaded from kaggle and store as dataframe object.

```{r echo=TRUE, message=TRUE, warning=FALSE, paged.print=TRUE}
#Import the data
supstore_df=read.csv('/Users/deenuy/Documents/Google Drive/Personal/CSDA1050-Capstone-Project-Repo/Datasets/superstore-sales/superstore.csv', stringsAsFactors = TRUE, header = T)

#supstore_df=read.csv('E:/Deenu/Personal Workspace/GoogleDrive/CSDA1050-Capstone-Project-Repo/Datasets/superstore-sales/superstore.csv', stringsAsFactors = TRUE, header = T)

#Display table with 5 rows
show_table(supstore_df, 5, isknit = FALSE)
```

Print the dimension of dataframe object.
```{r echo=FALSE}
#Display shape or size of dataframe table
sprintf("Dataframe size is: %d x %d", nrow(supstore_df), ncol(supstore_df))
```

# Data Preparation
Before we dive into exploratory analysis, it is important that the data is cleaned to extract meaningful insights. Let us address the following problems one by one as scope of data cleansing.

 * Converting to factor variables, numeric variables & handling dates
 * Removing least useful informations for downsizing the dataframe object
 * Analyze and handle missing values
 * Data trasformation, Checking Linearity, Skewness and generate plot histogram

### Converting to factor variables, numeric variables & handling dates 
Describe the data types for each column using function str(). 
```{r paged.print=TRUE}
#Summarize the data
str(supstore_df) ##to find data types
```

From above it is identified that data type needs to be fixed for attributes Order.date, ship.date and postal.code.
```{r}
#Data format conversion process
supstore_df$Order.Date = parse_date_time(supstore_df$Order.Date, "dmy")
supstore_df$Ship.Date = parse_date_time(supstore_df$Ship.Date, "dmy")
supstore_df$Postal.Code = as.factor(supstore_df$Postal.Code)
```

Generate the summary to each column and collating the statistical results to understand our data with mean, median, min, max and null values. 
```{r}
#Summarize the data
summary(supstore_df)
```

### Removing least useful informations for downsizing the dataframe object
In consideration of ethical machine learning framework, removing the customer name from our dataset to mask the identity.
```{r echo=TRUE}
#remove the columns
supstore_df = select(supstore_df, -"Customer.Name")

#list columns
names(supstore_df)
```

### Analyze and handle missing values
```{r}
#Find is NA count and null values function
gen_report_na_func = function(dataset) {
  summary.na.df = 
    dataset %>%
    map_dfr(~sum(is.na(.)))
  
  #Transpose the table
  summary.na.df = as.data.frame(t(as.matrix(summary.na.df)))
  setDT(summary.na.df, keep.rownames = "newname")
  names(summary.na.df)[1] = "column_names"
  names(summary.na.df)[2] = "cols"
  
  #Generate summary of NA in all the columns
  summary.na_tb = 
    summary.na.df %>%
    group_by(column_names) %>%
    summarise(na_count = sum(cols), na_pcnt = (sum(cols)/nrow(dataset))*100) %>%
    arrange(desc(na_count))
  
  #Round of to two digit in percentage
  summary.na_tb$na_pcnt = round(summary.na_tb$na_pcnt,2)
  
  # View(summary.na_tb)
  show_table(summary.na_tb, 10, isknit = TRUE)
}

#Generate missing NA report
gen_report_na_func(supstore_df)

```


```{r echo=TRUE}
#Adding columns for data and time individually.

supstore_df_new =
  supstore_df %>%
  mutate(Profit_pct = round( (Profit/Sales)*100 ,2),
         Ordered.Year = lubridate::year(Order.Date), 
         Ordered.Quarter = lubridate::quarter(Order.Date), 
         Ordered.Month = lubridate::month(Order.Date, label=TRUE, abbr=TRUE), 
         Ordered.Day = lubridate::day(Order.Date),
         Ordered.Week.day = lubridate::wday(Order.Date, week_start = 1, label=TRUE, abbr=TRUE),
         Delivery_time = difftime(Ship.Date, Order.Date, units = c("days"))
         )

#summary(supstore_df_new)
```

### Data trasformation, Checking Linearity, Skewness and generate plot histogram
Analyze the univariate outliers for the sales
```{r echo=TRUE, fig.height=6, fig.width=10, warning=FALSE}
# use a box plot to see if we have outliers
a = ggplot(supstore_df, aes(x = "Profit", y = Sales)) +
    geom_boxplot() +
    theme_function()

# use a Normality Distribution by Q-Q plot to see if we have outliers
#qqnorm(supstore_df$Sales)
#qqline(supstore_df$Sales)

#The fitdistr( ) function in the MASS package provides maximum-likelihood fitting of univariate distributions. The format is fitdistr(x, densityfunction) where x is the sample data and densityfunction is one of the following: "beta", "cauchy", "chi-squared", "exponential", "f", "gamma", "geometric", "log-normal", "lognormal", "logistic", "negative binomial", "normal", "Poisson", "t" or "weibull".
#fitdistr(Victim.Age, "normal")

# use a box plot to see if we have outliers
c = ggplot(supstore_df, aes(x = "Profit", y = Discount)) +
    geom_boxplot() +
    theme_function()

# Scatter plots (sp)
d = ggscatter(supstore_df_new, x = "Discount", y = "Profit",
                add = "reg.line",               # Add regression line
                conf.int = TRUE,                # Add confidence interval
                ) +
  theme_function() +
  stat_cor(aes(color = Profit), label.x = 1)# Add correlation coefficient


e = ggscatter(supstore_df_new, x = "Sales", y = "Discount",
                palette = "jco",
                size = 3, alpha = 0.6)+
  theme_function() +
  border()

ggarrange(a, c, d, e + rremove("x.text"), 
          labels = c("A", "C", "D", "E"),
          ncol = 2, nrow = 2, align = "hv")
```

### Analyzing the profit and discount by category

```{r}

ggplot(supstore_df_new, aes(x = Discount, y = Profit)) + 
  geom_point(alpha = 0.5) + 
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black"),
                     guide = FALSE) + 
  facet_grid(~Category)

```




### Analyzing the profit and discount by sub category

```{r fig.height=3, fig.width=12, paged.print=FALSE}
ggplot(supstore_df_new, aes(x = Discount, y = Profit)) + 
  geom_point(alpha = 0.5) + 
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black"),
                     guide = FALSE) + 
  facet_grid(~Sub.Category)
```

### Analyzing the profit and discount by customer segment

```{r}
ggplot(supstore_df_new, aes(x = Sales, y = Discount)) + 
  geom_point(alpha = 0.5) + 
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black"),
                     guide = FALSE) + 
  facet_grid(~Category)
```

### Analyze for any outliers in the sales by category
```{r}
ggplot(supstore_df_new, aes(x = Sub.Category, y = Profit)) + 
  geom_boxplot() + 
  theme_function() +
  xlab("Category")+ ylab("Profit (%)") 
```


# Data Exploration and Analysis

### Analysing the superstore customer orders by state or province in US
From top and bottow 5 states result and plot, we can see that the California, New York and Texas have high demand of customers with quantity of 1000+ orders and whereas states like Wyoming, West Virginia, North Dakota and Maine are in single digit orders. Does this data shows any significant opportunities for more promotional investments for marketing team?

```{r echo=TRUE}
#Group by function to count number of orders by state
supstore_df_bystate = 
    supstore_df_new %>%
    group_by(State) %>%
    summarise(Orders = n_distinct(Order.ID),
              Average.profit = round(mean(Profit)),
              Average.sales = round(mean(Sales))) %>%
    mutate(Avg.sales.pct = round( (Average.sales/sum(Average.sales) * 100), 1),
           Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
    arrange(desc(Orders))

show_table(supstore_df_bystate, 10, isknit = TRUE)
```

```{r fig.height=9, fig.width=13, message=FALSE, warning=FALSE}
#Plot the orders by state
ggplot(supstore_df_bystate, 
       aes(State, Orders, fill=State)) + 
  geom_bar(stat="identity") +
  theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=24, hjust= 0.5), 
          axis.text.x = element_text(size=12, angle=90)
          ) +
  scale_color_gradient2() +
  labs(x="US States",y="Orders Count",title="Distribution of Orders by State (2014 - 2017)") +
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")),vjust=0.8,size=3.5,color='black',angle = 90, fontface='bold')
```


```{r}
#Display least orders by state
knitr::kable(tail(supstore_df_bystate, 5))

#tail(supstore_df_bystate, 5)
```

### Analyze the shipping mode preferences for online orders 
We can see that the most commonly used ship mode is Standard Shipping and the least used is Same Day shipping.

```{r}
#Group by shipping mode 
supstore_df_byShipMode = 
  supstore_df %>%
  group_by(Ship.Mode) %>%
  summarise(Orders = n_distinct(Order.ID),
            Average.sales = round(mean(Sales))) %>%
  mutate(Avg.sales.pct = round( (Average.sales/sum(Average.sales) * 100), 1),
         Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
  arrange(desc(Orders.pct))

#View the results in table format
show_table(supstore_df_byShipMode, 5, isknit = TRUE)
```

Plotting the shipping mode for visualization
```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE}
# Create the barplot for orders placed by shipping mode preference
ggplot(supstore_df_byShipMode, 
       aes(Ship.Mode, Orders, fill=Ship.Mode)) + 
  geom_bar(stat="identity") +
  theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=16, hjust= 0.5), 
          axis.text.x = element_text(size=12, angle=90)
          ) +
  scale_fill_brewer(palette = "Greens") +
  coord_flip() +
  labs(x="Shipping Mode", y="Orders Count", title="Distribution of Orders by Shipping Mode (2014 - 2017)") +
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")),hjust = 1.8, vjust=0.8,size=3.5,color='black',angle = 0, fontface='bold')

```

### Analyzing top 5 customers for frequent orders
```{r}
#Group by customer ID
supstore_df_byCustomerId = 
  supstore_df %>%
  group_by(Customer.ID) %>%
  summarise(Orders = n_distinct(Order.ID)) %>%
  arrange(desc(Orders))

#View the results in table format
show_table(supstore_df_byCustomerId, 5, isknit = TRUE)
```

```{r echo=FALSE}
sprintf("Number of unique orders observed is %d", nrow(supstore_df_byCustomerId))
```

### Analyze the catergorize that Superstore retailer have available online.
We can observe that office supplies have 60% of the consumer orders placed online between 2014-2017.
```{r}
#Group by category of products
supstore_df_byCategory = 
  supstore_df %>%
  group_by(Category) %>%
  summarise(Orders = n_distinct(Order.ID)) %>%
  mutate(Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
  arrange(desc(Orders.pct))

#View the results in table format
show_table(supstore_df_byCategory, 5, isknit = TRUE)
```

Plot to visualize the orders by category
```{r}
pie_cat = plot_ly(supstore_df_byCategory,  labels = ~Category,  values = ~Orders,  type = 'pie')
pie_cat = pie_cat %>% layout(title = 'Distribution of customer orders by categories of products', 
                              xaxis = list(showgrid = FALSE,  zeroline = FALSE,  showticklabels = TRUE), 
                              yaxis = list(showgrid = FALSE,  zeroline = FALSE,  showticklabels = TRUE))

pie_cat #Plotting pie chart of categories
```


### Analyzing the subcategories of products
The top 10 results shows that there is more customer demands for home office products like binders, papers, phones, storage, chairs and accessories.
```{r}
#Group by sub category of products
supstore_df_bySubCategory = 
  supstore_df %>%
  group_by(Sub.Category, Category) %>%
  summarise(Orders = n(), Average.profit = round(mean(Profit)),
            Average.sales = round(mean(Sales))) %>%
  arrange(desc(Orders))

#View the results in table format
show_table(supstore_df_bySubCategory, 5, isknit = TRUE)
```

```{r echo=FALSE}
sprintf("Number of subcategory of products identified are %d", nrow(supstore_df_bySubCategory))
```


```{r fig.height=7, fig.width=10}
# Initiate a ggplot
ggplot(supstore_df_bySubCategory, aes(x = Orders, y = Sub.Category)) +
  geom_point(aes(color = Sub.Category, size = Average.profit), alpha = 0.7) +
  geom_text(aes(label = Sub.Category) ) + 
  theme(legend.position = "none") +
  scale_color_viridis_d(begin = 0) +
  scale_size(range = c(5, 70), guide = "none") +
  labs(title = "Profit analysis for Orders by Subcategory",
     x = "Orders",
     y = "Sub Category")
```

### Analyze the online superstore retail sales by region in United States
Our observation is, southern region of United states has slightly lower online sales. Also, we can see that most of the orders are from the west,   coinciding with the number of orders from California from our initial analysis. 

```{r}
#Group by region of products sales
supstore_df_byRegion = 
  supstore_df %>%
  group_by(Region) %>%
  summarise(Orders = n_distinct(Order.ID),
            Average.profit = round(mean(Profit)),
            Average.sales = round(mean(Sales))) %>%
  mutate(Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
  arrange(desc(Orders))

#View the results in table format
show_table(supstore_df_byRegion, 5, isknit = TRUE)
```

### Analyze the segements of customer orders by consumer type.
Below results shows the consumers are the most common customer segment with more than 50% of the total orders.
```{r}
#Group by segments of consumer type
supstore_df_bySegments = 
  supstore_df %>%
  group_by(Segment) %>%
  summarise(Orders = n_distinct(Order.ID),
            Average.profit = round(mean(Profit)),
            Average.sales = round(mean(Sales))) %>%
  mutate(Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
  arrange(desc(Orders))

#View the results in table format
show_table(supstore_df_bySegments, 5, isknit = TRUE)
```

Plotting pie chart of composition of segment
```{r eval=FALSE, fig.height=8, fig.width=14, include=FALSE}
library(RColorBrewer)
#make a coxcomb plot
a = ggplot(supstore_df_bySegments, aes(Segment, Orders, fill=Segment)) +
  geom_bar(stat="identity") +
      theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=14, hjust= 0.5), 
          axis.text.x = element_text(size=11, angle=0)
          ) +
  scale_fill_brewer(palette = "Greens") +
  coord_polar() +
  #labs(title="Distribution of orders according to consumer segments") +
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")),hjust=1.1,vjust=1.2,size=3.2,color='black',fontface='bold')

#make a coxcomb plot
b = ggplot(supstore_df_byRegion, aes(Region, Orders, fill=Region)) +
  geom_bar(stat="identity") +
      theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=14, hjust= 0.5), 
          axis.text.x = element_text(size=11, angle=0)
          ) +
  scale_color_gradient2() +
  coord_polar() +
  #labs(title="Distribution of Orders by Region (2014 - 2017)") + 
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")),hjust=-0.1,vjust=0.8,size=3.2,color='black',fontface='bold')


#make a coxcomb plot
c = ggplot(supstore_df_byCategory, aes(Category, Orders, fill=Category)) +
  geom_bar(stat="identity") +
      theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=14, hjust= 0.5), 
          axis.text.x = element_text(size=11, angle=0)
          ) +
  scale_fill_brewer(palette = "Blues") +
  coord_polar() +
  #labs(title="Distribution of Orders by Region (2014 - 2017)") + 
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")),hjust=-0.1,vjust=0.8,size=3.2,color='black',fontface='bold')

ggarrange(a, b, c, 
          labels = c("1.", "2.", "3."),
          ncol = 2, nrow = 2, align = "hv")
```


Plot for visualization of orders by segment
```{r fig.height=6, fig.width=6}

#make a coxcomb plot
ggplot(supstore_df_bySegments, aes(Segment, Orders, fill=Segment)) +
  geom_bar(stat="identity") +
      theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=14, hjust= 0.5), 
          axis.text.x = element_text(size=11, angle=0)
          ) +
  scale_fill_brewer(palette = "Greens") +
  coord_polar() +
  labs(title="Distribution of Orders by Customer Segment") +
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")),hjust=1.1,vjust=1.2,size=3.8,color='black',fontface='bold')

```


Plot for visualization of orders by region
```{r fig.height=6, fig.width=6}

#make a coxcomb plot
ggplot(supstore_df_byRegion, aes(Region, Orders, fill=Region)) +
  geom_bar(stat="identity") +
      theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=14, hjust= 0.5), 
          axis.text.x = element_text(size=11, angle=0)
          ) +
  scale_color_gradient2() +
  coord_polar() +
  labs(title="Distribution of Orders by Region") + 
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")),hjust=-0.1,vjust=0.8,size=3.8,color='black',fontface='bold')

```


Plot for visualization of orders by category
```{r fig.height=6, fig.width=6}

#make a coxcomb plot
ggplot(supstore_df_byCategory, aes(Category, Orders, fill=Category)) +
  geom_bar(stat="identity") +
      theme_bw() + 
    theme(legend.position="none",
          plot.title = element_text(size=14, hjust= 0.5), 
          axis.text.x = element_text(size=11, angle=0)
          ) +
  scale_fill_brewer(palette = "Blues") +
  coord_polar() +
  labs(title="Distribution of Orders by Category") + 
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")),hjust=-0.2,vjust=-0.9,size=3.8,color='black',fontface='bold')

```


### Analyzing the quantity
We wanted to see the quantity of products that customer frequently places online order. 

```{r}
#Group by sub category of products
supstore_df_byQuantity = 
  supstore_df %>%
  group_by(Quantity, Product.Name) %>%
  summarise(Frequency = n()) %>%
  mutate(Percentage = round( (Frequency/sum(Frequency) * 100), 1)) %>%
  arrange(desc(Frequency))

#View the results in table format
show_table(supstore_df_byQuantity, 10, isknit = TRUE)
```


### Average sales of orders per state
Creating new dataframe consisiting only of State and sales. We can see that Wyoming is an outlier which could be removed from the analysis for clustering.
```{r}
df_state_sales = supstore_df %>%
  select(State,  Sales) #Creating new dataframe consisting only of State Column and Sales
  
df_state_avg_sales = df_state_sales %>%
  group_by(State) %>%
  summarise(mean_sales = mean(Sales)) #Creating another dataframe of State and sales

bar_avg_sales = plot_ly(y = df_state_avg_sales$mean_sales,  x = df_state_avg_sales$State,  type = "bar") #Plotting bar graph of frequency of ship time
bar_avg_sales = bar_avg_sales%>% layout(title = 'Average sales price of each state', 
                                         xaxis = list(showgrid = FALSE,  zeroline = FALSE,  showticklabels = TRUE), 
                                         yaxis = list(showgrid = FALSE,  zeroline = FALSE,  showticklabels = TRUE))
bar_avg_sales #Plotting bar graph of average sales price for each state
```

### Exploring the number of days required to ship orders
We can see that 49.5% of the orders shipped within 4-5 days.
```{r}
#Group by region of products sales
supstore_df_byShippingDuration = 
  supstore_df_new %>%
  group_by(Delivery_time) %>%
  summarise(Orders = n_distinct(Order.ID),
            Average.profit = round(mean(Profit)),
            Average.sales = round(mean(Sales))) %>%
  mutate(Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
  arrange(desc(Orders))

#View the results in table format
show_table(supstore_df_byShippingDuration, 10, isknit = TRUE)
```

### Analysing customer orders, saleas and profit by year on year
```{r echo=TRUE, fig.height=5, fig.width=8, message=TRUE, warning=TRUE}
#orders grouped by year
supstore_df_byYoY = 
  supstore_df_new %>%
  group_by(Sub.Category, Ordered.Year) %>%
  summarise(Orders = n(),
            Average.profit = round(mean(Profit)),
            Average.sales = round(mean(Sales))
            ) %>%
  mutate(Orders_pcnt = round( (Orders/sum(Orders) * 100), 1),
         Orders_YoY_rise =  round((Orders - lag(Orders))/lag(Orders) * 100),
         Sales_YoY_rise =  round((Average.sales - lag(Average.sales))/lag(Average.sales) * 100),
         Profit_YoY_rise =  round((Average.profit - lag(Average.profit))/lag(Average.profit) * 100)
         )

#View the results in table format
show_table(supstore_df_byYoY, 10, isknit = TRUE)
```

```{r fig.height=4, fig.width=8}
a = ggplot(data=supstore_df_new, aes(x=Ordered.Year, y=Sales, fill = Category)) +
  geom_bar(stat="identity", position=position_dodge())+
  scale_fill_brewer(palette="Paired")+
  theme_minimal()

b = ggplot(data=supstore_df_new, aes(x=Ordered.Year, y=Sales, fill = Segment)) +
  geom_bar(stat="identity", position=position_dodge())+
  scale_fill_brewer(palette="Paired")+
  theme_minimal()

ggarrange(a, b,
          labels = c("A", "B"),
          ncol = 2, nrow = 1, align = "hv")

```


### Analysing sales by state and city
```{r echo=TRUE, fig.height=5, fig.width=10, message=TRUE, warning=TRUE}
#Overall sales and profit grouped by state and city
supstore_df_sales_byCity = 
  supstore_df_new %>%
  group_by(State, City) %>%
  summarise(Average.sales = round(mean(Sales)), Average.profit = round(mean(Profit))) %>%
  arrange(desc(Average.sales, Average.profit))

#View the results in table format
show_table(supstore_df_sales_byCity, 10, isknit = TRUE)

```


### Analysing sales and profit by consumer segment and category
```{r echo=TRUE, fig.height=5, fig.width=10, message=TRUE, warning=TRUE}
#Overall sales and profit grouped by consumer segment and category
supstore_df_sales_bCategory_byConsumer = 
  supstore_df_new %>%
  group_by(Segment, Category) %>%
  summarise(Average.profit = round(mean(Profit)), Average.sales = round(mean(Sales))) %>%
  arrange(desc(Segment, Category))

#View the results in table format
show_table(supstore_df_sales_bCategory_byConsumer, 10, isknit = TRUE)
```

### Analyze the product orders trend by week days
```{r}
#Overall orders grouped by week days 
supstore_df_orders_byweekdays = 
  supstore_df_new %>%
  group_by(Ordered.Week.day) %>%
  summarise(Orders = n_distinct(Order.ID), Average.sales = round(mean(Sales))) %>%
  mutate(Avg.sales.pct = round( (Average.sales/sum(Average.sales) * 100), 1)) %>%
  arrange((Ordered.Week.day))

#View the results in table format
show_table(supstore_df_orders_byweekdays, 10, isknit = TRUE)
```

```{r}
#Plot the orders by state
ggplot(supstore_df_orders_byweekdays, 
       aes(Ordered.Week.day, Orders, fill=Ordered.Week.day)) + 
  geom_bar(stat="identity") +
  theme_function() +
  scale_color_gradient2() +
  labs(x="Week Days",y="Orders Count",title="Distribution of Orders by Week Days (2014 - 2017)")+
  geom_text(aes(label=paste0(Avg.sales.pct,"%",sep=" ")),hjust=0.3,vjust=1.8,size=3.8,color='white',fontface='bold')

```


Visualizing in bubbl chart
```{r fig.height=6, fig.width=10}
# Initiate a ggplot
ggplot(supstore_df_orders_byweekdays, aes(x = Ordered.Week.day, y = Orders)) +
  geom_point(aes(color = Ordered.Week.day, size = Orders), alpha = 0.7) +
  geom_text(aes(label = Ordered.Week.day) ) + 
  scale_color_manual(name = "Week days", values = c("#ED5B67", "#E7B800", "#FC4E07","#168B98", "#596F7E", "#fd8f24","#919c4c")) +
  scale_size(range = c(5, 60), guide = "none") +
  labs(title = "Distribution of Orders by Week Days",
       x = "Orders",
       y = "Week Days")
```

### Analyze the product orders trend by Months
```{r}
#Overall orders grouped by week days 
supstore_df_orders_byMonths = 
  supstore_df_new %>%
  group_by(Ordered.Month) %>%
  summarise(Orders = n_distinct(Order.ID),
            Average.profit = round(mean(Profit)),
            Average.sales = round(mean(Sales))) %>%
  mutate(Avg.sales.pct = round( (Average.sales/sum(Average.sales) * 100), 1),
         Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
  arrange((Ordered.Month))

#View the results in table format
show_table(supstore_df_orders_byMonths, 10, isknit = TRUE)
```


# Data Modeling - Clustering Analysis
We wanted to investigate any further patterns or segments in the order transactions, profit, discount and sales. To do so, we will be employing the hierarchical and k-medoid clustering methods.

Cluster analysis is a multivariate data mining technique whose goal is to group objects based on a set of user selected characteristics. The clusters should exhibit high internal homogeneity and high external
heterogeneity, meaning that when plotted geometrically, objects within clusters should be very close together and clusters will be far apart.

Due to the categorical nature of our dependent variables, time (hourly range) and area names, we used hierarchical clustering to analyse our objective. The aim was to set up a hierarchy of clusters, not only to
categorize the data into a set number of clusters, but also to have a sequence of groupings at different levels.

## Hierarchical Clustering:
The hierarchical clustering technique is one of the most popular clustering techniques for unsupervised machine learning algorithm.

The Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. It does not require us to pre-specify the number of clusters to be generated as is required by the kmeans
approach. Furthermore, hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram. 

Hierarchical clustering can be divided into two main types: agglomerative and divisive. Agglomerative clustering: It’s also known as AGNES (Agglomerative Nesting). It works in a bottom-up
manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the
algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This
procedure is iterated until all points are members of just one single big cluster (root).

Divisive hierarchical clustering: 
It’s also known as DIANA (Divise Analysis) and it works in a top-down manner. The algorithm is an inverse order of AGNES. It begins with the root, in which all objects are included in a single cluster. At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster.

Agglomerative clustering is good at identifying small clusters. Divisive hierarchical clustering is good at identifying large clusters. In our implementation we have followed both methods to determine right clustering group.

Clustering Distance Measures:
The classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix. There are many methods to calculate this distance information. 

In our implementation we have used the Euclidean method for computing the distance (Euclidean Method: It is the distance two points in either the plane or 3-dimensional space measures the length of a segment
connecting the two points). 

Below ordered steps are followed to accomplish the objective.
1. Calculate the distance matrix for hierarchical clustering
2. Choose a linkage method (single, complete, average and ward) to determine dissimilarity between two clusters of observations by correlation distant coefficient.
    i. Method of single linkage or nearest neighbour. Proximity between two clusters is the proximity between their two closest objects.
    ii. Method of complete linkage or farthest neighbour. Proximity between two clusters is the proximity between their two most distant objects
    iii. Method of between-group average linkage. Proximity between two clusters is the arithmetic mean of all the proximities between the objects of one, on one side, and the objects of the other, on the other side.
    iv. Ward’s method, or minimal increase of sum-of-squares. Proximity between two clusters is the magnitude by which the summed square in their joint cluster will be greater than the combined summed square in these two clusters.
3. Plot the data as a dendrogram
4. Plot the data using fviz_cluster function from the factoextra package to visualize the result in a scatter plot

### Data Transformation for Hierarchical Clustering

To perform a cluster analysis in R, generally, the data should be prepared as follows:
  1. Rows are observations (individuals) and columns are variables
  2. Any missing value in the data must be removed or estimated.
  3. The data must be standardized (i.e., scaled) to make variables comparable. Recall that, standardization consists of transforming the variables such that they have mean zero and standard deviation one.

```{r message=FALSE, warning=FALSE}

#Binning or adding categorical values based on ranges in continous data
setDT(supstore_df_new)[Profit_pct > 100, Profit_pct_range := "> 100%"]
supstore_df_new[Profit_pct >= 0 & Profit_pct <10, Profit_pct_range := "0 to 10%"]
supstore_df_new[Profit_pct >= 10 & Profit_pct <20, Profit_pct_range := "10% to 20%"]
supstore_df_new[Profit_pct >= 20 & Profit_pct <30, Profit_pct_range := "20% to 30%"]
supstore_df_new[Profit_pct >= 30 & Profit_pct <40, Profit_pct_range := "30% to 40%"]
supstore_df_new[Profit_pct >= 40 & Profit_pct <50, Profit_pct_range := "40% to 50%"]
supstore_df_new[Profit_pct >= 50 & Profit_pct <60, Profit_pct_range := "50% to 60%"]
supstore_df_new[Profit_pct <= 0 & Profit_pct > -10, Profit_pct_range := "0 to -10%"]
supstore_df_new[Profit_pct <= -10 & Profit_pct > -20, Profit_pct_range := "-10% to -20%"]
supstore_df_new[Profit_pct <= -20 & Profit_pct > -30, Profit_pct_range := "-20% to -30%"]
supstore_df_new[Profit_pct <= -30 & Profit_pct > -40, Profit_pct_range := "-30% to -40%"]
supstore_df_new[Profit_pct <= -40 & Profit_pct > -50, Profit_pct_range := "-40% to -50%"]
supstore_df_new[Profit_pct <= -50 & Profit_pct > -100, Profit_pct_range := "-50% to -100"]
supstore_df_new[Profit_pct <= -100 & Profit_pct > -150, Profit_pct_range := "-100% to -150%"]
supstore_df_new[Profit_pct <= -150 & Profit_pct > -300, Profit_pct_range := "-150% to -300%"]

#Numeric conversion
supstore_df_new$Profit_pct_range = as.factor(supstore_df_new$Profit_pct_range)
supstore_df_new$Profit_pct = as.numeric(supstore_df_new$Profit_pct)

#Hierarchical Clustering Dataset by Area Name
hcus_by_Profit_pct = 
supstore_df_new %>% 
  select(Profit_pct_range, Profit_pct, Sales, Quantity, Discount, Customer.ID, Sub.Category, Segment) %>%
  arrange(Profit_pct_range)

hcus_by_Profit_pct2 =
hcus_by_Profit_pct %>% 
  #filter(Victim.Age <= 85) %>%
  select(Profit_pct_range, Sales, Quantity, Discount, Customer.ID, Sub.Category, Segment) %>%
  group_by(Profit_pct_range) %>%
  summarise(total_orders = n(), avg.sales = round(mean(Sales)), avg.Discount = round(mean(Discount))) %>%
  arrange(desc(Profit_pct_range))

#Converting Area.Name column as rownames
row.names(hcus_by_Profit_pct2) = hcus_by_Profit_pct2$Profit_pct_range
hcus_by_Profit_pct2[1] = NULL

# Applying the transformation with scaling function to centers and/or scales the columns of a numeric matrix.
hcus_by_Profit_pct3 = scale(hcus_by_Profit_pct2)

```

### Analyze the product by profit margins
```{r include=FALSE}
#Overall orders grouped by profit
supstore_df_byProfit = 
  supstore_df_new %>%
  group_by(Profit_pct_range, Category) %>%
  summarise(Orders = n_distinct(Order.ID)) %>%
  mutate(Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
  arrange(desc(Orders.pct))

#View the results in table format
show_table(supstore_df_byProfit, 10, isknit = TRUE)
```

```{r fig.height=4, fig.width=8}
#Plot the orders by state
library(viridis)
ggplot(supstore_df_byProfit,
       aes(reorder(Profit_pct_range, -Orders), Orders, fill=Category)) + 
  geom_bar(position="stack", stat="identity") +
  scale_fill_manual(values = c("#0073C2FF", "#EFC000FF", "#999999"))+
  theme_pubclean() +
  labs(x="Profit Range", y="Orders", title="Distribution of Orders by Profit%") +
  geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")), size=3.2, color='black', fontface='bold', position = position_stack(vjust = 0.5))

```


## Hierarchical Clustering Analysis - Unsupervised learning - by Profit Range

Implementing Hierarchical Clustering using Agglomerative Clustering using four linkage methods:

We can perform agglomerative Hierarchical Clustering with hclust in R. First we compute the dissimilarity values with dist ‘Euclidean Method’ and then feed these values into hclust and specify the agglomeration method to be used (i.e. “complete”, “average”, “single”, “ward.D”). We can then plot the dendrogram.

```{r fig.height=6, fig.width=12, message=TRUE, warning=FALSE, include=FALSE}
#Required Libraries
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms

#set seed
set.seed(175)
#install.packages('package_name', dependencies = TRUE)

#### Agglomerative clustering: It’s also known as AGNES (Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). The result is a tree which can be plotted as a dendrogram.

# Compute the dissimilarity matrix values with dist 
d = dist(hcus_by_Profit_pct3, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 = hclust(d, method = "average" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)

```

We can see the differences these approaches from above dendrograms.

We have used agnes function to get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

Following is a custom function to loop all four linkage methods and generate the coefficient to determine the strongest clustering method using ANGES (Agglomerative Clustering in R).

```{r echo=TRUE, fig.height=6, fig.width=12, message=TRUE, warning=FALSE}

# function to compute coefficient for different agglomerative clustering methods
methods = c( "average", "single", "complete", "ward")
names(methods) = c( "average", "single", "complete", "ward")

ac = function(x) {
  agnes(hcus_by_Profit_pct3, method = x)$ac
}

map_dbl(methods, ac)
```

Here we see that Ward’s method identifies the strongest clustering structure as distant coefficient is 0.93.

We will continue Agglomerative Hierarchical Clustering using ward method to plot the Dendrogram and scatter plot.
```{r fig.height=6, fig.width=12, message=TRUE, warning=FALSE}

#printf("Here we see that Ward’s method identifies the strongest clustering structure")

# We will continue Agglomerative Hierarchical Clustering using ward method
# Ward's method
d2 = dist(hcus_by_Profit_pct3, method = "euclidean")

hc5 = hclust(d2, method = "ward.D2")

# Cut tree into 4 groups
sub_grp = cutree(hc5, k = 5)

# Rotate the plot and remove default theme
#install.packages("ggdendro")
library("ggdendro")
ggdendrogram(hc5, rotate = TRUE, theme_dendro = FALSE) + 
  labs(title = "Hierarchical Clustering Dendrogram - Profit % Segmentation",
       x = "Profit % Range",
       y = "Distance")

```


Here is the enhanced Visualization Of Dendrogram. Draws easily beautiful dendrograms using either ggplot2. Provides also an option for drawing a circular dendrogram tree.
```{r fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
library(factoextra)

#the arguments main, sub, xlab, ylab to change plot titles as follow:
fviz_dend(hc5, k = 5,                 # Cut in four groups
          cex = 0.5,                 # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE,  # color labels by groups
          horiz = TRUE,             # Flip the chart
          ggtheme = theme_classic(),     # Change theme
          ylab = "Height",
          xlab = "Profit % Range",
          main = "Hierarchical Clustering Dendrogram - Profit % Segmentation",
          type = c("circular") #type = c("rectangle", "circular", "phylogenic"),
          
          )
```

## Data Modeling - K-Medoids Clustering
An alternative approach to hierarchical clustering is k-medoids clustering for identifying groups in the dataset. It does not require us to pre-specify the number of clusters to be generated as the calculation is done by the Sillhouette width method. We will first measure the gower distance and then then the sillhouette width, based on which we will decide how many clusters are optimal. Then we will attempt the clustering.

```{r eval=FALSE, include=FALSE}
library(Rtsne)

#Creating copy of data sets with just the necessary columns
supstop_df_clust = select(supstore_df_new, Segment, Category, Sales, Discount, Profit) 

#Calculating Gower Distance
gower_dist = daisy(supstop_df_clust, metric = "gower") 
gower_mat = as.matrix(gower_dist)

#'Print most similar and most different customers
similar_customers = supstop_df_clust[which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]), arr.ind = TRUE)[1, ], ]

#view(similar_customers)
distinct_customers = supstop_df_clust[which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]), arr.ind = TRUE)[1, ], ]

#Using silhouette coefficient method to find the optimal number of clusters
sil_width = c(NA)
for(i in 2:8){  
  pam_fit = pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] = pam_fit$silinfo$avg.width  
}

plot(1:8, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:8, sil_width)

#As per the below graph, 7 seems the best number of clusters as the silhouette coefficient is the maximum for 7 clusters.
```

As suggested by the Silhouette width, we have 7 clusters as the most optimal number of clusters. 

```{r eval=FALSE, include=FALSE}

#Derived from the silhouette width
j = 7
pam_fit = pam(gower_dist, diss = TRUE, j)
pam_results = supstop_df_clust%>%
  mutate(cluster = pam_fit$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results$the_summary

```

Now we will try to plot the 7 clusters

```{r eval=FALSE, include=FALSE}
#Visualization in lower dimensional space for the 7 clusters formed
set.seed(200)

tsne_obj = Rtsne(gower_dist, is_distance = TRUE)
tsne_data = tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))
```

However, plotting the clusters, shows that the clusters formed are not ideal. Therefore, instead of creating 7 clusters, we would be creating 3 clusters instead.

```{r}
knitr::include_graphics("/Users/deenuy/Documents/Google Drive/Personal/CSDA1050-Capstone-Project-Repo/Final Submission/Copy of K-Medoid-7-Clusters.png")
```


```{r eval=FALSE, include=FALSE}

j = 3 #Derived from the silhouette width
pam_fit2 = pam(gower_dist, diss = TRUE, j)
pam_results2 = supstop_df_clust%>%
  mutate(cluster = pam_fit2$clustering) %>%
  group_by(cluster) %>%
  do(the_summary2 = summary(.))
pam_results2$the_summary2

```

After the 3 clusters have been created and summarized, we would attempt to visualize the clusters.

```{r eval=FALSE, include=FALSE}
#Visualization in lower dimensional space for the 3 clusters formed
set.seed(200)

tsne_obj2 = Rtsne(gower_dist, is_distance = TRUE)
tsne_data2 = tsne_obj2$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit2$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data2) +
  geom_point(aes(color = cluster))
```

The 3 clusters have been plotted, however, the results were not optimal in determining the cluster groups due to no clear delineation of clusters/grouping.

In the earlier steps we have analyzed clustering models (Hierarchical and K-medoids) to dertermine any segments from given data and feed to appriori algorithm. But we have observed that adding segmentation is to appriori is condensing the outcome of recommendation list due to size of dataset which is less than 10000 transactions approx. Hence we have decided to exclude the segmentation from our algorithm implementation and applied appriori technique on whole dataset.

## Data Modeling: Associative Rule Mining (Apriori)

The next step is to convert the data frame into basket format, based on the Order_ID. New Dataframe df_itemList created below. The `ddply()` function checks the names of the products and pivots them into one single line separating them by commas.

Here we select the `apriori algorithm` as the modeling technique for our `recommendation analysis`. At this point, after much research, we are assuming that the algorithm will work well with our updated data frame. We have pivoted the table to show the full order details on one line with the order no. and all associated products purchased separated by commas.

The data doesn’t have any missing value which should facilitate the analysis. To increase the efficiency of our analysis we also specify the support, confidence, and lift of our analysis.  

The `support has been set at 0.001` to measure the number of transactions in which an itemset appears at least `0.1%` of the time.

  * Ex. Supp(x) = number of transactions in which x appears/Total number of transactions

The `confidence is set at 0.5` to measure the number of transactions in which the two or more items appear at least `50%` of the time.

  * Ex. Conf(X à Y) = Supp(X ∪ Y)/Supp(X)

The `lift` determined by greater than 1 and higher value to show how likely an item is purchased when a second item is purchased.

  * Ex. Lift(X à Y) =  Supp(X ∪ Y)/Supp(X) * Supp(Y)
  
For this project, we have implemented apriori in two steps, one for frequent itemset generation with support (0.025) and confidence (50%), and second step to generate the candidate rules from each frequent itemset. The candidate rule provides the lhs (selected dataset item) and rhs (recommended corresponding dataset item list) components to prompt customers with a recommended list of items on the selected one.

Finally, we run the algorithm.

```{r eval=FALSE, include=FALSE}
if(sessionInfo()['basePkgs']=="dplyr" | sessionInfo()['otherPkgs']=="dplyr"){
    detach(package:dplyr, unload=TRUE)
}
library(plyr)
#install.packages("arules", dependencies=TRUE)
library(arules)

df_itemList_bskt3 = ddply(supstore_df_new, 
                          c("Order.ID"),
                          function(supstore_df_new) paste(supstore_df_new$Product.Name, collapse = "|"))

dim(df_itemList_bskt3)

#For Basket Analysis, we do not need Order.ID and Order.Date
df_itemList_bskt3$Order.ID = NULL
df_itemList_bskt3$Order.Date = NULL
df_itemList_bskt3$Category = NULL

#Rename column headers for ease of use
colnames(df_itemList_bskt3) = c("itemList")

#Write dataframe to a csv file using write.csv()
write.csv(df_itemList_bskt3,"ItemList.csv", quote=FALSE, row.names = TRUE)

#Using the read.transactions() functions, we can read the file ItemList.csv and convert it to a transaction format
txn = read.transactions(file = "ItemList.csv", rm.duplicates = TRUE, format = "basket", sep = "|", cols = 1)
#View(txn@itemInfo$labels)

#Removing the Quotes which are introduced in read.transactions 
txn@itemInfo$labels = gsub("\"", "", txn@itemInfo$labels)
#View(txn)
head(txn@itemInfo$labels, 40)

#Execute the apriori algorithm on the transactions by specifying minimum values for support and confidence. Support is an indication of how frequently the items appear in the data. Confidence indicates the number of times the if-then statements are found true. A third metric, called lift, can be used to compare confidence with expected confidence
basket_rules = apriori(txn, parameter = list(sup = 0.00025, conf = 0.3, target="rules"))
```


### Assess the model

Once the algorithm implemented, we used the `inspect()` function to manipulate the arguments to verify if we can get to a more accurate recommendation.

Now, we will concatenate the product names as per the order number.
```{R eval=FALSE, include=FALSE}
dim(basket_rules@lhs@itemInfo)
#Print the association rules using function inspect(). Note that if package attached in the session, it creates a conflict with the arules package. Thus, we need to check and detach the package.
if(sessionInfo()['basePkgs']=="tm" | sessionInfo()['otherPkgs']=="tm"){
    detach(package:tm, unload=TRUE)
}

inspect(basket_rules)

#Alternative to inspect() is to convert rules to a dataframe and then use View()
df_basket = as(basket_rules, "data.frame")
dim(df_basket)

#View(df_basket)
#convert to datframe and view; optional
df_basket = as(basket_rules,"data.frame")
df_basket$confidence = df_basket$confidence * 100

# split lhs and rhs into two columns
library(reshape2)
df_basket = transform(df_basket, rules = colsplit(rules, pattern = "=>", names = c("lhs","rhs")))

# Remove curly brackets around rules
df_basket$rules$lhs = gsub("[[:punct:]]", "", df_basket$rules$lhs)
df_basket$rules$rhs = gsub("[[:punct:]]", "", df_basket$rules$rhs)

# convert to chracter
df_basket$rules$lhs = as.character(df_basket$rules$lhs)
df_basket$rules$rhs = as.character(df_basket$rules$rhs)

#inspect(df_basket)
library(stringi)
library(dplyr)

#Remove duplicates from LHS
df_basket_filtered = 
    df_basket %>%
    distinct(df_basket$rules$lhs, .keep_all = TRUE)

#Remove column
df_basket_filtered = select(df_basket_filtered, -'df_basket$rules$lhs')
```

Manual testing with input "Acco Hanging Data Binders" in LHS and get RHS as recommended frequent item
```{r eval=FALSE, message=TRUE, warning=FALSE, include=FALSE}
#Input the product name in LHS to show RHS
df_basket_filtered$rules %>%
    filter(stri_detect_fixed(lhs, "Acco Hanging Data Binders")) %>% 
    select(rhs)
```

### Visualization for Apriori Algorithm 

Now, ploting the vizualiztions. These functions can be accessed form the arulesViz library.

```{r eval=FALSE, message=TRUE, warning=FALSE, include=FALSE}
plot(basket_rules) #Simple plot of the Association rules

```

```{r eval=FALSE, message=TRUE, warning=FALSE, include=FALSE}
plot(basket_rules, method = "grouped", control = list(k = 5))

```

```{r eval=FALSE, message=TRUE, warning=FALSE, include=FALSE}
plot(basket_rules, method = "graph", control=list(type="items"))

```

```{r eval=FALSE, message=TRUE, warning=FALSE, include=FALSE}
#plot(basket_rules, method = "paracoord", control=list(alpha=.5, reorder=TRUE))

```

Inputs for Shiny Application Dashboard
```{r eval=FALSE, include=FALSE}

#Numeric conversion
supstore_df_new$Profit_pct_range = as.factor(supstore_df_new$Profit_pct_range)
supstore_df_new$Profit_pct = as.numeric(supstore_df_new$Profit_pct)

#List for customer segments
customer_seg_list = as.list(unique(supstore_df_new$Segment))

#List for Region segments
region_seg_list = as.list(unique(supstore_df_new$Region))

#List for category segments
category_seg_list = as.list(unique(supstore_df_new$Category))

#Average Basket Price
avg.basket.price =
    supstore_df_new %>%
    group_by(Customer.ID) %>%
    summarise(avg.price = round(mean(Sales), 2))
    
#Total sales revenue
sales.revenue =
    supstore_df_new %>%
    summarise(total_revenue = round(sum(Sales)))


```

### Shiny Dashboard Implementation
Shiny is an R package that makes it easy to build interactive web applications (apps) straight from R.

We have developed the Dashboard serving two functions:
* A descriptive dashboard to simulate the performance of various segments
* A recommender system to show the consumer preferences and purchasing behaviours

The dashboard is hosted on a secured Shiny cloud platform to simplify integration, improve agility, and ease the burden of implementation for Superstore IT resources. 

```{r eval=FALSE, include=FALSE}
# Define UI for application that draws a histogram
ui = dashboardPage(
    #The title of the dashboard
    dashboardHeader(
        title = "SuperStore Dashboard",
        titleWidth = 230
    ),
    dashboardSidebar(
        #Sidebar content of the dashboard
        sidebarMenu(
            menuItem("Analytic Dashboard", tabName = "dashboard", icon = icon("dashboard")),
            menuItem("Consumer Preference Recommender", tabName = "widgets", icon = icon("bar-chart-o"))
        )
    ),
    dashboardBody(
        tabItems(
            tabItem(
                tabName = "dashboard", 
                h2("Analytic Dashboard"),
                fluidRow(
                    valueBoxOutput("value1"),
                    valueBoxOutput("value2"),
                    valueBoxOutput("value3")
                ),
                fluidRow(
                  box(
                    title = "Performance by Segment",
                    status = "primary",
                    solidHeader = TRUE,
                    collapsible = TRUE,
                    width = 12,
                    selectInput(inputId = "segmentInput",
                                label = "Customer Segment Type",
                                choices = c("Segment","Category","Region"),
                                selected = "Category"),
                    plotOutput("profitBySegment", width = "100%")
                  )
                ),
                fluidRow(),
                fluidRow(
                      column(1,
                             selectInput(inputId = "xInput",
                                         label = "X-Axis",
                                         choices = c("Sales","Discount","Days to Ship" = "Discount"),
                                         selected = "Sales"),
                             br()
                      ),
                      column(2,
                             #offset = 1,
                             selectInput(inputId = "yInput",
                                         label = "Y-Axis",
                                         choices = c("Profit Ratio" = "Profit"),
                                         selected = "Profit")
                      ),
                      column(3,
                             selectInput(inputId = "segmentTypeInput",
                                         label = "Segment Type",
                                         choices = c("Segment","Category","Sub Category" = "Sub.Category"),
                                         selected = "Sub.Category")
                      ),
                      column(4,
                             #offset = 1,
                             selectInput(inputId = "regionInput",
                                         label = "Region",
                                         choices = c("US",
                                                     "Central",
                                                     "East", 
                                                     "West", 
                                                     "South"),
                                         selected = "US")
                      ),
                      title = "Rename this box",
                      status = "primary",
                      solidHeader = TRUE, 
                      collapsible = TRUE,
                      width = 12,
                      plotOutput("bubbleChart", width = "100%")
                ),
                fluidRow(),
                br(),
                br()
                # fluidRow(
                #     box(
                #         title = "Profit % by Segments Table",
                #         status = "primary",
                #         solidHeader = TRUE, 
                #         collapsible = TRUE,
                #         #Input section
                #         selectInput(inputId = "segmentTabInput",
                #                     label = "Select Segment",
                #                     choices = c("Segment","Category","Region"),
                #                     selected = "Segment"),
                #         #Output section
                #         tableOutput("profitBySegmentTable")
                #     )
                # )
            ),
            
            tabItem(
                tabName = "widgets",
                h2("Consumer Preference Recommender"),
                fluidRow(),
                fluidRow(
                    box(
                        title = "Consumer Preference Recommender System",
                        status = "primary",
                        solidHeader = TRUE, 
                        collapsible = TRUE,
                        
                        #Input section
                        selectInput("input_item", "Item #1", choices = df_basket_filtered$rules$lhs, selected = df_basket_filtered$rules$lhs[2], selectize = TRUE),
                        h3("Other Items you might be Interested:"),
                        #Output section
                        tableOutput("productRecommender")
                    )
                )
            )
        )
    )
)

server = function(input, output) {
    
    observeEvent(input$switchtab, {
    newtab = switch(input$tabs,
      "dashboard" = "widgets",
      "widgets" = "dashboard"
    )
    updateTabItems(session, "tabs", newtab)
    })

    #Construct the dataset for first box inputs
    supstore_df_byProfit = reactive({
        
            supstore_df_new %>%
            dplyr::group_by(Profit_pct_range, !!sym(input$segmentInput)) %>%
            dplyr::summarise(Orders = n_distinct(Order.ID)) %>%
            dplyr::mutate(Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
            arrange(desc(Orders.pct))
    })
    
    #Plot for first box
    output$profitBySegment = renderPlot({
        ggplot(supstore_df_byProfit(),
               aes(reorder(Profit_pct_range, -Orders), Orders, fill=!!sym(input$segmentInput))) + 
            geom_bar(position="stack", stat="identity") +
            scale_fill_manual(values = cbPalette)+
            theme_pubclean() +
            labs(x="Profit Range", y="Orders", title="Distribution of Orders by Profit%") +
            geom_text(aes(label=paste0(Orders.pct,"%",sep=" ")), size=3.2, color='black', fontface='bold', position = position_stack(vjust = 0.5))
        })
    
    #Construct the dataset for second box inputs
    supstore_df_byBox2Input =  reactive({
      
        supstore_df_new %>%
        dplyr::filter(Order.Date > "2014-01-01" & Order.Date <"2017-12-30") %>%
        dplyr::group_by(!!sym(input$segmentTypeInput)) %>%
        dplyr::summarise(
                  Orders = n_distinct(Order.ID),
                  Average.y = round(mean(!!sym(input$yInput))),
                  Average.x = round(mean(!!sym(input$xInput))),
                  total_x = round(sum(!!sym(input$xInput)))
                  ) %>%
        dplyr::mutate(
               yx_ratio = round((Average.y/Average.x) * 100)
               )
    })

    #Plot for second box - Initiate a ggplot bubble chart
    output$bubbleChart = renderPlot({

      ggplot(supstore_df_byBox2Input(),
             aes(x = total_x, y = yx_ratio)) +
          geom_point(aes(color = !!sym(input$segmentTypeInput), size = Orders), alpha = 0.6) +
          geom_text(aes(label = !!sym(input$segmentTypeInput)) ) + 
          labs(x='Sales', y='Profit Ratio') +
          theme(legend.position="none") +
          scale_size(range = c(1, 50)) +
          scale_y_continuous(labels = scales::label_percent(scale = 1, suffix = "%")) +
          scale_x_continuous(labels = scales::unit_format(unit = "k", prefix = "$", scale = 1e-3))
    })

    #segmentTabInput
    supstore_df_byProfit2 = reactive({
        
        supstore_df_new %>%
            dplyr::group_by(Profit_pct_range, !!sym(input$segmentTabInput)) %>%
            dplyr::summarise(Orders = n_distinct(Order.ID)) %>%
            dplyr::mutate(Orders.pct = round( (Orders/sum(Orders) * 100), 1)) %>%
            arrange(desc(Orders.pct))
        })   
    
    #Plot2
    output$profitBySegmentTable = renderTable({
        head(supstore_df_byBox2Input(), 5)
        })
    
    #creating the valueBoxOutput content
    output$value1 = renderValueBox({
        valueBox(
            formatC(sales.revenue$total_revenue, format="d", big.mark=','),
            paste('Sales Revenue'),
            icon = icon("usd",lib='glyphicon'),
            color = "purple")  
    })
    output$value2 = renderValueBox({ 
        valueBox(
            formatC(length(unique(supstore_df_new$Customer.ID)), format="d", big.mark=','),
            'Total Customers',
            icon = icon("stats",lib='glyphicon'),
            color = "green")  
    })
    output$value3 = renderValueBox({
        valueBox(
            formatC(avg.basket.price$avg.price, format="d", big.mark=','),
            paste('Average Basket Spend'),
            icon = icon("usd",lib='glyphicon'),
            color = "yellow")   
    })
    
    #RHS Recommender Result
    results = reactive({
        df_basket_filtered$rules %>%
        filter(stri_detect_fixed(lhs, input$input_item)) %>%
        select(rhs)
        })
    
    #Recommender Output
    output$productRecommender = renderTable({
        head(results())
        })
}

# Run the application 
shinyApp(ui = ui, server = server)

```

Calculating execution run time

```{r eval=FALSE, message=FALSE, include=FALSE}
#stop time execution completion
stop_time  =  Sys.time()
total_execution_time  =  start_time - stop_time
print(total_execution_time)
```
